import os
import glob
from dotenv import load_dotenv
import weaviate
from weaviate.classes.init import Auth
from langchain_weaviate import WeaviateVectorStore
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
# from langchain_huggingface import HuggingFaceEmbeddings

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å .env
load_dotenv()

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
weaviate_api_key = os.getenv("WEAVIATE_API_KEY")
data_folder = "E:/workspace/langchain-study/data"  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô path ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
collection_name = "LangchainStudy"

# ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Weaviate
weaviate_client = weaviate.connect_to_local(
    host="localhost",
    port=8080,
    grpc_port=50051,
    auth_credentials=Auth.api_key(weaviate_api_key)
)

if weaviate_client.is_ready():
    print("‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Weaviate ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
else:
    print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Weaviate ‡πÑ‡∏î‡πâ! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Docker Container ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")
    exit()

#‡∏Ç‡∏≠‡∏á HuggingFace

# model_name = "BAAI/bge-m3"
# model_kwargs = {"device": "cpu"}
# encode_kwargs = {"normalize_embeddings": True}
# hf = HuggingFaceEmbeddings(
#     model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs
# )

# embeddings = hf

# ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings model
embeddings = OllamaEmbeddings(model="bge-m3")

# ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå .txt ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥)
text_files = set()  # ‡πÉ‡∏ä‡πâ set ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥

# ‡πÉ‡∏ä‡πâ case-insensitive pattern
for pattern in ["*.txt", "*.TXT"]:
    files = glob.glob(os.path.join(data_folder, pattern))
    for file in files:
        # normalize path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥
        normalized_path = os.path.normpath(file.lower())
        if os.path.exists(file):  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á
            text_files.add(file)

# ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô list
text_files = list(text_files)

if not text_files:
    print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå .txt ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå")
    weaviate_client.close()
    exit()

print(f"üìÅ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {len(text_files)} ‡πÑ‡∏ü‡∏•‡πå:")
for file in sorted(text_files):  # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏î‡∏π‡∏á‡πà‡∏≤‡∏¢
    print(f"  - {os.path.basename(file)} ({os.path.getsize(file)} bytes)")

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö collection ‡πÄ‡∏Å‡πà‡∏≤ ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà
try:
    if weaviate_client.collections.exists(collection_name):
        print(f"‚ö†Ô∏è  Collection '{collection_name}' ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß")
        response = input("‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (y/N): ")
        if response.lower() == 'y':
            weaviate_client.collections.delete(collection_name)
            print(f"üóëÔ∏è  ‡∏•‡∏ö Collection '{collection_name}' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
        else:
            print("‚ùå ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£")
            weaviate_client.close()
            exit()
except Exception as e:
    print(f"‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö collection ‡πÑ‡∏î‡πâ: {e}")

# ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå
all_chunks = []
processed_content = set()  # ‡πÄ‡∏Å‡πá‡∏ö hash ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥
text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)

for file_path in text_files:
    try:
        print(f"\nüîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {os.path.basename(file_path)}")
        
        # ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ã‡πâ‡∏≥
        for doc in documents:
            content_hash = hash(doc.page_content)
            if content_hash in processed_content:
                print(f"   ‚ö†Ô∏è  ‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå {os.path.basename(file_path)} - ‡∏Ç‡πâ‡∏≤‡∏°")
                continue
            
            processed_content.add(content_hash)

         
        
        # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        chunks = text_splitter.split_documents(documents)
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö chunk
        unique_chunks = []
        chunk_contents = set()
        
        for chunk in chunks:
            chunk_hash = hash(chunk.page_content)
            if chunk_hash not in chunk_contents:
                chunk_contents.add(chunk_hash)
                unique_chunks.append(chunk)
        
        all_chunks.extend(unique_chunks)
        
        if len(chunks) != len(unique_chunks):
            print(f"   üìù ‡πÅ‡∏ö‡πà‡∏á‡πÑ‡∏î‡πâ {len(chunks)} chunks (‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á‡∏ã‡πâ‡∏≥: {len(unique_chunks)} chunks)")
        else:
            print(f"   üìù ‡πÅ‡∏ö‡πà‡∏á‡πÑ‡∏î‡πâ {len(unique_chunks)} chunks")
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå {file_path}: {e}")

if not all_chunks:
    print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥ embedding")
    weaviate_client.close()
    exit()

print(f"\nüìä ‡∏™‡∏£‡∏∏‡∏õ:")
print(f"  - ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(text_files)} ‡πÑ‡∏ü‡∏•‡πå")
print(f"  - Chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(all_chunks)} chunks")
print(f"  - ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏£‡∏ß‡∏°: {sum(len(chunk.page_content) for chunk in all_chunks):,} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")

print(f"\nüöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {len(all_chunks)} chunks...")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á vector store
try:
    db = WeaviateVectorStore.from_documents(
        all_chunks,
        embeddings,
        client=weaviate_client,
        index_name=collection_name,
        text_key="text"
    )
    print("‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô collection
    collection = weaviate_client.collections.get(collection_name)
    total_objects = collection.aggregate.over_all(total_count=True).total_count
    print(f"üìà ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô collection: {total_objects}")
    weaviate_client.close()
    
except Exception as e:
    print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings: {e}")
    weaviate_client.close()
    exit()