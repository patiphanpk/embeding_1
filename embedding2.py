import os
import glob
from dotenv import load_dotenv
import weaviate
from weaviate.classes.init import Auth
from langchain_weaviate import WeaviateVectorStore
from langchain.schema import Document
from langchain_ollama import OllamaEmbeddings
# from langchain_huggingface import HuggingFaceEmbeddings

# === Import splitter ‡∏à‡∏≤‡∏Å doc_chunk_edit2.py ===
from doc_chunk_edit2 import AcademicDocumentSplitter

load_dotenv()

weaviate_api_key = os.getenv("WEAVIATE_API_KEY")
data_folder = "E:/workspace/langchain-study/data" 
collection_name = "LangchainStudy"

weaviate_client = weaviate.connect_to_local(
    host="localhost",
    port=8080,
    grpc_port=50051,
    auth_credentials=Auth.api_key(weaviate_api_key)
)

if weaviate_client.is_ready():
    print("‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Weaviate ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
else:
    print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Weaviate ‡πÑ‡∏î‡πâ! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Docker Container ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")
    exit()

embeddings = OllamaEmbeddings(model="bge-m3")

# ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå .txt ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
text_files = set()
for pattern in ["*.txt", "*.TXT"]:
    files = glob.glob(os.path.join(data_folder, pattern))
    for file in files:
        if os.path.exists(file):
            text_files.add(file)

text_files = list(text_files)

if not text_files:
    print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå .txt ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå")
    weaviate_client.close()
    exit()

print(f"üìÅ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {len(text_files)} ‡πÑ‡∏ü‡∏•‡πå")

# === ‡πÉ‡∏ä‡πâ AcademicDocumentSplitter ===
splitter = AcademicDocumentSplitter()

total_chunks = 0
total_chars = 0

for file_path in sorted(text_files):
    print(f"\nüîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {os.path.basename(file_path)}")
    try:
        content = splitter.read_file(file_path)
        if not content.strip():
            print(f"‚ö†Ô∏è  ‡πÑ‡∏ü‡∏•‡πå {file_path} ‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ")
            continue

        chunks = splitter.split_document(content, source_file=file_path)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Document objects
        docs = [
            Document(
                page_content=chunk["content"],
                metadata={k: v for k, v in chunk.items() if k != "content"}
            )
            for chunk in chunks
        ]

        print(f"   üìù ‡πÑ‡∏î‡πâ {len(docs)} chunks ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå {os.path.basename(file_path)}")

        # === push ‡∏ó‡∏µ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Ç‡πâ‡∏≤ Weaviate ===
        db = WeaviateVectorStore.from_documents(
            docs,
            embeddings,
            client=weaviate_client,
            index_name=collection_name,
            text_key="content"
        )

        total_chunks += len(docs)
        total_chars += sum(len(doc.page_content) for doc in docs)

    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå {file_path}: {e}")

# === ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ===
collection = weaviate_client.collections.get(collection_name)
total_objects = collection.aggregate.over_all(total_count=True).total_count

print(f"\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:")
print(f"  - ‡πÑ‡∏ü‡∏•‡πå: {len(text_files)}")
print(f"  - Chunks: {total_chunks}")
print(f"  - ‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏ß‡∏°: {total_chars:,} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
print(f"üìà ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô collection: {total_objects}")

weaviate_client.close()